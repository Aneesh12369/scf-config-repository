environment='local'

project_dir="/opt/sonetel-ai/AI_Crawler"
depth=3
cache_file='/tmp/crawler.db'
threads=10
#Don't crawl if the number of urls in the website exceeds max_urls.
max_urls=100

# [StatsD]
statsd_host = '127.0.0.1'
statsd_port = 8125

#[Kafka Config]
kafka_host="192.168.122.188"
kafka_port="9092"
consume_topic="ai-crawler-input"
produce_topic="ai-extractor-input"
consumer_timeout=10000000

#[S3 Bucket]
s3_bucket="ai-crawled-data"
s3_access_key="AKIAJSA5IB7ZLZHLJ3WQ"
s3_secret_key="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"

#[Graylog]
#graylog_dev_server = "34.194.26.199"
graylog_dev_server = "192.168.18.243"
graylog_dev_port = 12202
graylog_prod_server = "translogs-collector.sonetel.com"
graylog_prod_port = 12231

